
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> 
> require(pbapply)
Loading required package: pbapply
> source('code/model/emersonscott_model.R')
Loading required package: data.table
Loading required package: tidyverse
â”€â”€ [1mAttaching packages[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.0 â”€â”€
[32mâœ“[39m [34mggplot2[39m 3.3.0     [32mâœ“[39m [34mpurrr  [39m 0.3.4
[32mâœ“[39m [34mtibble [39m 3.0.4     [32mâœ“[39m [34mdplyr  [39m 1.0.2
[32mâœ“[39m [34mtidyr  [39m 1.0.2     [32mâœ“[39m [34mstringr[39m 1.4.0
[32mâœ“[39m [34mreadr  [39m 1.3.1     [32mâœ“[39m [34mforcats[39m 0.5.0
â”€â”€ [1mConflicts[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
[31mx[39m [34mdplyr[39m::[32mbetween()[39m   masks [34mdata.table[39m::between()
[31mx[39m [34mdplyr[39m::[32mfilter()[39m    masks [34mstats[39m::filter()
[31mx[39m [34mdplyr[39m::[32mfirst()[39m     masks [34mdata.table[39m::first()
[31mx[39m [34mdplyr[39m::[32mlag()[39m       masks [34mstats[39m::lag()
[31mx[39m [34mdplyr[39m::[32mlast()[39m      masks [34mdata.table[39m::last()
[31mx[39m [34mpurrr[39m::[32mtranspose()[39m masks [34mdata.table[39m::transpose()
Loading required package: lsa
Loading required package: SnowballC
Loading required package: statnet
Loading required package: tergm
Loading required package: ergm
Loading required package: network
network: Classes for Relational Data
Version 1.16.1 created on 2020-10-06.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Martina Morris, University of Washington
                    Skye Bender-deMoll, University of Washington
 For citation information, type citation("network").
 Type help("network-package") to get started.


ergm: version 3.10.4, created on 2019-06-10
Copyright (c) 2019, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Martina Morris, University of Washington
                    with contributions from
                    Li Wang
                    Kirk Li, University of Washington
                    Skye Bender-deMoll, University of Washington
                    Chad Klumb
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm").

NOTE: Versions before 3.6.1 had a bug in the implementation of the bd()
constriant which distorted the sampled distribution somewhat. In
addition, Sampson's Monks datasets had mislabeled vertices. See the
NEWS and the documentation for more details.

NOTE: Some common term arguments pertaining to vertex attribute and
level selection have changed in 3.10.0. See terms help for more
details. Use â€˜options(ergm.term=list(version="3.9.4"))â€™ to use old
behavior.

Loading required package: networkDynamic

networkDynamic: version 0.10.1, created on 2020-01-16
Copyright (c) 2020, Carter T. Butts, University of California -- Irvine
                    Ayn Leslie-Cook, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Skye Bender-deMoll, University of Washington
                    with contributions from
                    Zack Almquist, University of California -- Irvine
                    David R. Hunter, Penn State University
                    Li Wang
                    Kirk Li, University of Washington
                    Steven M. Goodreau, University of Washington
                    Jeffrey Horner
                    Martina Morris, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("networkDynamic").


tergm: version 3.6.1, created on 2019-06-12
Copyright (c) 2019, Pavel N. Krivitsky, University of Wollongong
                    Mark S. Handcock, University of California -- Los Angeles
                    with contributions from
                    David R. Hunter, Penn State University
                    Steven M. Goodreau, University of Washington
                    Martina Morris, University of Washington
                    Nicole Bohme Carnegie, New York University
                    Carter T. Butts, University of California -- Irvine
                    Ayn Leslie-Cook, University of Washington
                    Skye Bender-deMoll
                    Li Wang
                    Kirk Li, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("tergm").

Loading required package: ergm.count

ergm.count: version 3.4.0, created on 2019-05-15
Copyright (c) 2019, Pavel N. Krivitsky, University of Wollongong
                    with contributions from
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm.count").

NOTE: The form of the term â€˜CMPâ€™ has been changed in version 3.2 of
â€˜ergm.countâ€™. See the news or help('CMP') for more information.

Loading required package: sna
Loading required package: statnet.common

Attaching package: â€˜statnet.commonâ€™

The following object is masked from â€˜package:baseâ€™:

    order

sna: Tools for Social Network Analysis
Version 2.6 created on 2020-10-5.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
 For citation information, type citation("sna").
 Type help(package="sna") to get started.

Loading required package: tsna

statnet: version 2019.6, created on 2019-06-13
Copyright (c) 2019, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Skye Bender-deMoll
                    Martina Morris, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("statnet").


There are updates for the following statnet packages on CRAN:
      Installed ReposVer Built  
ergm  "3.10.4"  "3.11.0" "3.6.3"
tergm "3.6.1"   "3.7.0"  "3.6.3"
Restart R and use "statnet::update_statnet()" to get the updates.
Loading required package: igraph

Attaching package: â€˜igraphâ€™

The following objects are masked from â€˜package:snaâ€™:

    betweenness, bonpow, closeness, components, degree, dyad.census,
    evcent, hierarchy, is.connected, neighborhood, triad.census

The following objects are masked from â€˜package:networkâ€™:

    %c%, %s%, add.edges, add.vertices, delete.edges, delete.vertices,
    get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite,
    is.directed, list.edge.attributes, list.vertex.attributes,
    set.edge.attribute, set.vertex.attribute

The following objects are masked from â€˜package:dplyrâ€™:

    as_data_frame, groups, union

The following objects are masked from â€˜package:purrrâ€™:

    compose, simplify

The following object is masked from â€˜package:tidyrâ€™:

    crossing

The following object is masked from â€˜package:tibbleâ€™:

    as_data_frame

The following objects are masked from â€˜package:statsâ€™:

    decompose, spectrum

The following object is masked from â€˜package:baseâ€™:

    union

Loading required package: multinets
multinets was developed by Neylson Crepalde as an extension to 'igraph'
Loading required package: tidygraph

Attaching package: â€˜tidygraphâ€™

The following object is masked from â€˜package:igraphâ€™:

    groups

The following object is masked from â€˜package:statsâ€™:

    filter

Loading required package: intergraph
> require(parallel)
Loading required package: parallel
> require(doParallel)
Loading required package: doParallel
Loading required package: foreach

Attaching package: â€˜foreachâ€™

The following objects are masked from â€˜package:purrrâ€™:

    accumulate, when

Loading required package: iterators
> 
> cores = floor(detectCores() / 1.2)
> cl = makeCluster(cores)
> reps = 2e4
> registerDoParallel(cl)
> 
> clusterEvalQ(cl,expr =  source('code/model/emersonscott_model.R'))
[[1]]
[[1]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[1]]$visible
[1] FALSE


[[2]]
[[2]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[2]]$visible
[1] FALSE


[[3]]
[[3]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[3]]$visible
[1] FALSE


[[4]]
[[4]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[4]]$visible
[1] FALSE


[[5]]
[[5]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[5]]$visible
[1] FALSE


[[6]]
[[6]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[6]]$visible
[1] FALSE


[[7]]
[[7]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[7]]$visible
[1] FALSE


[[8]]
[[8]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[8]]$visible
[1] FALSE


[[9]]
[[9]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[9]]$visible
[1] FALSE


[[10]]
[[10]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[10]]$visible
[1] FALSE


[[11]]
[[11]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[11]]$visible
[1] FALSE


[[12]]
[[12]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[12]]$visible
[1] FALSE


[[13]]
[[13]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[13]]$visible
[1] FALSE


[[14]]
[[14]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[14]]$visible
[1] FALSE


[[15]]
[[15]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[15]]$visible
[1] FALSE


[[16]]
[[16]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[16]]$visible
[1] FALSE


[[17]]
[[17]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[17]]$visible
[1] FALSE


[[18]]
[[18]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[18]]$visible
[1] FALSE


[[19]]
[[19]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[19]]$visible
[1] FALSE


[[20]]
[[20]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[20]]$visible
[1] FALSE


[[21]]
[[21]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[21]]$visible
[1] FALSE


[[22]]
[[22]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[22]]$visible
[1] FALSE


[[23]]
[[23]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[23]]$visible
[1] FALSE


[[24]]
[[24]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[24]]$visible
[1] FALSE


[[25]]
[[25]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[25]]$visible
[1] FALSE


[[26]]
[[26]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[26]]$visible
[1] FALSE


[[27]]
[[27]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[27]]$visible
[1] FALSE


[[28]]
[[28]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[28]]$visible
[1] FALSE


[[29]]
[[29]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[29]]$visible
[1] FALSE


[[30]]
[[30]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[30]]$visible
[1] FALSE


[[31]]
[[31]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[31]]$visible
[1] FALSE


[[32]]
[[32]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[32]]$visible
[1] FALSE


[[33]]
[[33]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[33]]$visible
[1] FALSE


[[34]]
[[34]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[34]]$visible
[1] FALSE


[[35]]
[[35]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[35]]$visible
[1] FALSE


[[36]]
[[36]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[36]]$visible
[1] FALSE


[[37]]
[[37]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[37]]$visible
[1] FALSE


[[38]]
[[38]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[38]]$visible
[1] FALSE


[[39]]
[[39]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[39]]$visible
[1] FALSE


[[40]]
[[40]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[40]]$visible
[1] FALSE


[[41]]
[[41]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[41]]$visible
[1] FALSE


[[42]]
[[42]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[42]]$visible
[1] FALSE


[[43]]
[[43]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[43]]$visible
[1] FALSE


[[44]]
[[44]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[44]]$visible
[1] FALSE


[[45]]
[[45]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[45]]$visible
[1] FALSE


[[46]]
[[46]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[46]]$visible
[1] FALSE


[[47]]
[[47]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[47]]$visible
[1] FALSE


[[48]]
[[48]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[48]]$visible
[1] FALSE


[[49]]
[[49]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[49]]$visible
[1] FALSE


[[50]]
[[50]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[50]]$visible
[1] FALSE


[[51]]
[[51]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[51]]$visible
[1] FALSE


[[52]]
[[52]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[52]]$visible
[1] FALSE


[[53]]
[[53]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[53]]$visible
[1] FALSE


[[54]]
[[54]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[54]]$visible
[1] FALSE


[[55]]
[[55]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[55]]$visible
[1] FALSE


[[56]]
[[56]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[56]]$visible
[1] FALSE


[[57]]
[[57]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[57]]$visible
[1] FALSE


[[58]]
[[58]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[58]]$visible
[1] FALSE


[[59]]
[[59]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[59]]$visible
[1] FALSE


[[60]]
[[60]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[60]]$visible
[1] FALSE


[[61]]
[[61]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[61]]$visible
[1] FALSE


[[62]]
[[62]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[62]]$visible
[1] FALSE


[[63]]
[[63]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[63]]$visible
[1] FALSE


[[64]]
[[64]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[64]]$visible
[1] FALSE


[[65]]
[[65]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[65]]$visible
[1] FALSE


[[66]]
[[66]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[66]]$visible
[1] FALSE


[[67]]
[[67]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[67]]$visible
[1] FALSE


[[68]]
[[68]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[68]]$visible
[1] FALSE


[[69]]
[[69]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[69]]$visible
[1] FALSE


[[70]]
[[70]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[70]]$visible
[1] FALSE


[[71]]
[[71]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[71]]$visible
[1] FALSE


[[72]]
[[72]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[72]]$visible
[1] FALSE


[[73]]
[[73]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[73]]$visible
[1] FALSE


[[74]]
[[74]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[74]]$visible
[1] FALSE


[[75]]
[[75]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[75]]$visible
[1] FALSE


[[76]]
[[76]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[76]]$visible
[1] FALSE


[[77]]
[[77]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[77]]$visible
[1] FALSE


[[78]]
[[78]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[78]]$visible
[1] FALSE


[[79]]
[[79]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[79]]$visible
[1] FALSE


[[80]]
[[80]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[80]]$visible
[1] FALSE


> 
> incentive.set = list(c(0.1,0.5),c(0.3,0.7),c(0.5,0.9),c(0.1,0.9))
> 
> res = foreach(i = 1:reps) %dopar% {
+   simulation.control = list(stakeholders = 50,regulators = 0, convenors = 0 ,
+                             incentive.set = sample(incentive.set,1)[[1]],
+                             uncertainty = runif(1,min = 0.25,2.25), 
+                             n_pieces = 1,
+                             min.payout = 0,
+                             max.payout = 10,n_issues = 100,
+                             number.of.issues.to.join = 2,
+                             beta = 0.9,alpha = 0.1, 
+                             t = 50,perturb.time = 15,perturb.type = NULL,
+                             CGselector='betweenness',behavior = "consistent")
+   tryCatch({EmersonScottModel(simulation.control = simulation.control)},error = function(e) NULL)
+ }
> 
> saveRDS(res,paste0('../bucket_mount/verde_scratch/baserun.',reps/1e3,'k.RDS'))
> rm(res);gc()
          used  (Mb) gc trigger  (Mb) max used  (Mb)
Ncells 2140198 114.3    5568330 297.4  5568330 297.4
Vcells 3576103  27.3   14408888 110.0 15655620 119.5
> stopCluster(cl)
> 
> 
> 
> 
