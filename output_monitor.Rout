
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> 
> require(pbapply)
Loading required package: pbapply
> source('code/model/emersonscott_model.R')
Loading required package: data.table
Loading required package: tidyverse
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──
✓ ggplot2 3.3.3     ✓ purrr   0.3.4
✓ tibble  3.1.2     ✓ dplyr   1.0.2
✓ tidyr   1.1.2     ✓ stringr 1.4.0
✓ readr   1.4.0     ✓ forcats 0.5.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
x dplyr::between()   masks data.table::between()
x dplyr::filter()    masks stats::filter()
x dplyr::first()     masks data.table::first()
x dplyr::lag()       masks stats::lag()
x dplyr::last()      masks data.table::last()
x purrr::transpose() masks data.table::transpose()
Loading required package: lsa
Loading required package: SnowballC
Loading required package: statnet
Loading required package: tergm
Loading required package: ergm
Loading required package: network
network: Classes for Relational Data
Version 1.16.1 created on 2020-10-06.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Martina Morris, University of Washington
                    Skye Bender-deMoll, University of Washington
 For citation information, type citation("network").
 Type help("network-package") to get started.


ergm: version 3.11.0, created on 2020-10-14
Copyright (c) 2020, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, UNSW Sydney
                    Martina Morris, University of Washington
                    with contributions from
                    Li Wang
                    Kirk Li, University of Washington
                    Skye Bender-deMoll, University of Washington
                    Chad Klumb
                    Michał Bojanowski, Kozminski University
                    Ben Bolker
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm").

NOTE: Versions before 3.6.1 had a bug in the implementation of the bd()
constraint which distorted the sampled distribution somewhat. In
addition, Sampson's Monks datasets had mislabeled vertices. See the
NEWS and the documentation for more details.

NOTE: Some common term arguments pertaining to vertex attribute and
level selection have changed in 3.10.0. See terms help for more
details. Use ‘options(ergm.term=list(version="3.9.4"))’ to use old
behavior.

Loading required package: networkDynamic

networkDynamic: version 0.10.1, created on 2020-01-16
Copyright (c) 2020, Carter T. Butts, University of California -- Irvine
                    Ayn Leslie-Cook, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Skye Bender-deMoll, University of Washington
                    with contributions from
                    Zack Almquist, University of California -- Irvine
                    David R. Hunter, Penn State University
                    Li Wang
                    Kirk Li, University of Washington
                    Steven M. Goodreau, University of Washington
                    Jeffrey Horner
                    Martina Morris, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("networkDynamic").


tergm: version 3.7.0, created on 2020-10-15
Copyright (c) 2020, Pavel N. Krivitsky, UNSW Sydney
                    Mark S. Handcock, University of California -- Los Angeles
                    with contributions from
                    David R. Hunter, Penn State University
                    Steven M. Goodreau, University of Washington
                    Martina Morris, University of Washington
                    Nicole Bohme Carnegie, New York University
                    Carter T. Butts, University of California -- Irvine
                    Ayn Leslie-Cook, University of Washington
                    Skye Bender-deMoll
                    Li Wang
                    Kirk Li, University of Washington
                    Chad Klumb
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("tergm").

Loading required package: ergm.count

ergm.count: version 3.4.0, created on 2019-05-15
Copyright (c) 2019, Pavel N. Krivitsky, University of Wollongong
                    with contributions from
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm.count").

NOTE: The form of the term ‘CMP’ has been changed in version 3.2 of
‘ergm.count’. See the news or help('CMP') for more information.

Loading required package: sna
Loading required package: statnet.common

Attaching package: ‘statnet.common’

The following object is masked from ‘package:base’:

    order

sna: Tools for Social Network Analysis
Version 2.6 created on 2020-10-5.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
 For citation information, type citation("sna").
 Type help(package="sna") to get started.

Loading required package: tsna

statnet: version 2019.6, created on 2019-06-13
Copyright (c) 2019, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Skye Bender-deMoll
                    Martina Morris, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("statnet").


There are updates for the following statnet packages on CRAN:
               Installed ReposVer Built  
ergm           "3.11.0"  "4.0.1"  "3.6.3"
ergm.count     "3.4.0"   "4.0.2"  "3.6.3"
network        "1.16.1"  "1.17.1" "3.6.3"
networkDynamic "0.10.1"  "0.11.0" "3.6.3"
statnet.common "4.4.1"   "4.5.0"  "3.6.3"
tsna           "0.3.1"   "0.3.3"  "3.6.3"
Restart R and use "statnet::update_statnet()" to get the updates.
Loading required package: igraph

Attaching package: ‘igraph’

The following objects are masked from ‘package:sna’:

    betweenness, bonpow, closeness, components, degree, dyad.census,
    evcent, hierarchy, is.connected, neighborhood, triad.census

The following objects are masked from ‘package:network’:

    %c%, %s%, add.edges, add.vertices, delete.edges, delete.vertices,
    get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite,
    is.directed, list.edge.attributes, list.vertex.attributes,
    set.edge.attribute, set.vertex.attribute

The following objects are masked from ‘package:dplyr’:

    as_data_frame, groups, union

The following objects are masked from ‘package:purrr’:

    compose, simplify

The following object is masked from ‘package:tidyr’:

    crossing

The following object is masked from ‘package:tibble’:

    as_data_frame

The following objects are masked from ‘package:stats’:

    decompose, spectrum

The following object is masked from ‘package:base’:

    union

Loading required package: multinets
multinets was developed by Neylson Crepalde as an extension to 'igraph'
Loading required package: EnvStats
Registered S3 method overwritten by 'EnvStats':
  method   from
  plot.gof ergm

Attaching package: ‘EnvStats’

The following objects are masked from ‘package:stats’:

    predict, predict.lm

The following object is masked from ‘package:base’:

    print.default

Loading required package: tidygraph

Attaching package: ‘tidygraph’

The following object is masked from ‘package:igraph’:

    groups

The following object is masked from ‘package:stats’:

    filter

Loading required package: intergraph
Loading required package: reshape2

Attaching package: ‘reshape2’

The following object is masked from ‘package:tidyr’:

    smiths

The following objects are masked from ‘package:data.table’:

    dcast, melt

> require(parallel)
Loading required package: parallel
> require(doParallel)
Loading required package: doParallel
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
> 
> cores = floor(detectCores() - 2)
> cl = makeCluster(cores)
> reps = 10000
> registerDoParallel(cl)
> 
> clusterEvalQ(cl,expr =  source('code/model/emersonscott_model.R'))
[[1]]
[[1]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[1]]$visible
[1] FALSE


[[2]]
[[2]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[2]]$visible
[1] FALSE


[[3]]
[[3]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[3]]$visible
[1] FALSE


[[4]]
[[4]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[4]]$visible
[1] FALSE


[[5]]
[[5]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[5]]$visible
[1] FALSE


[[6]]
[[6]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[6]]$visible
[1] FALSE


[[7]]
[[7]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[7]]$visible
[1] FALSE


[[8]]
[[8]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[8]]$visible
[1] FALSE


[[9]]
[[9]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[9]]$visible
[1] FALSE


[[10]]
[[10]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[10]]$visible
[1] FALSE


[[11]]
[[11]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[11]]$visible
[1] FALSE


[[12]]
[[12]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[12]]$visible
[1] FALSE


[[13]]
[[13]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[13]]$visible
[1] FALSE


[[14]]
[[14]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[14]]$visible
[1] FALSE


[[15]]
[[15]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[15]]$visible
[1] FALSE


[[16]]
[[16]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[16]]$visible
[1] FALSE


[[17]]
[[17]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[17]]$visible
[1] FALSE


[[18]]
[[18]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[18]]$visible
[1] FALSE


[[19]]
[[19]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[19]]$visible
[1] FALSE


[[20]]
[[20]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[20]]$visible
[1] FALSE


[[21]]
[[21]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[21]]$visible
[1] FALSE


[[22]]
[[22]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[22]]$visible
[1] FALSE


[[23]]
[[23]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[23]]$visible
[1] FALSE


[[24]]
[[24]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[24]]$visible
[1] FALSE


[[25]]
[[25]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[25]]$visible
[1] FALSE


[[26]]
[[26]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[26]]$visible
[1] FALSE


[[27]]
[[27]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[27]]$visible
[1] FALSE


[[28]]
[[28]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[28]]$visible
[1] FALSE


[[29]]
[[29]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[29]]$visible
[1] FALSE


[[30]]
[[30]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[30]]$visible
[1] FALSE


[[31]]
[[31]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[31]]$visible
[1] FALSE


[[32]]
[[32]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[32]]$visible
[1] FALSE


[[33]]
[[33]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[33]]$visible
[1] FALSE


[[34]]
[[34]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[34]]$visible
[1] FALSE


[[35]]
[[35]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[35]]$visible
[1] FALSE


[[36]]
[[36]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[36]]$visible
[1] FALSE


[[37]]
[[37]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[37]]$visible
[1] FALSE


[[38]]
[[38]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[38]]$visible
[1] FALSE


[[39]]
[[39]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[39]]$visible
[1] FALSE


[[40]]
[[40]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[40]]$visible
[1] FALSE


[[41]]
[[41]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[41]]$visible
[1] FALSE


[[42]]
[[42]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[42]]$visible
[1] FALSE


[[43]]
[[43]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[43]]$visible
[1] FALSE


[[44]]
[[44]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[44]]$visible
[1] FALSE


[[45]]
[[45]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[45]]$visible
[1] FALSE


[[46]]
[[46]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[46]]$visible
[1] FALSE


[[47]]
[[47]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[47]]$visible
[1] FALSE


[[48]]
[[48]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[48]]$visible
[1] FALSE


[[49]]
[[49]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[49]]$visible
[1] FALSE


[[50]]
[[50]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[50]]$visible
[1] FALSE


[[51]]
[[51]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[51]]$visible
[1] FALSE


[[52]]
[[52]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[52]]$visible
[1] FALSE


[[53]]
[[53]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[53]]$visible
[1] FALSE


[[54]]
[[54]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[54]]$visible
[1] FALSE


[[55]]
[[55]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[55]]$visible
[1] FALSE


[[56]]
[[56]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[56]]$visible
[1] FALSE


[[57]]
[[57]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[57]]$visible
[1] FALSE


[[58]]
[[58]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[58]]$visible
[1] FALSE


[[59]]
[[59]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[59]]$visible
[1] FALSE


[[60]]
[[60]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[60]]$visible
[1] FALSE


[[61]]
[[61]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[61]]$visible
[1] FALSE


[[62]]
[[62]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[62]]$visible
[1] FALSE


[[63]]
[[63]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[63]]$visible
[1] FALSE


[[64]]
[[64]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[64]]$visible
[1] FALSE


[[65]]
[[65]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[65]]$visible
[1] FALSE


[[66]]
[[66]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[66]]$visible
[1] FALSE


[[67]]
[[67]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[67]]$visible
[1] FALSE


[[68]]
[[68]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[68]]$visible
[1] FALSE


[[69]]
[[69]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[69]]$visible
[1] FALSE


[[70]]
[[70]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[70]]$visible
[1] FALSE


[[71]]
[[71]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[71]]$visible
[1] FALSE


[[72]]
[[72]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[72]]$visible
[1] FALSE


[[73]]
[[73]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[73]]$visible
[1] FALSE


[[74]]
[[74]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[74]]$visible
[1] FALSE


[[75]]
[[75]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[75]]$visible
[1] FALSE


[[76]]
[[76]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[76]]$visible
[1] FALSE


[[77]]
[[77]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[77]]$visible
[1] FALSE


[[78]]
[[78]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createAttribute(simulation.control = simulation.control, 
        agents = agents, skill = "incentive.range")
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    agent.best.guesses = shared.info.matrix = private.info.matrix = cgr.wouldpayout.matrix = issue.matrix = incentive.matrix = expected.contrib.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL, issue.alignment = NULL, cgr.contributions = NULL)
    info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
        ag = agents, u = simulation.control$uncertainty, n_pieces = simulation.control$n_pieces), 
        simplify = F)
    for (t in 1:simulation.control$t) {
        agent_issue_dummy = matrix(NA, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        agent_issue_dummy[, cgr$issues] <- 1
        agent_issue_dummy[, -cgr$issues] <- 0
        agent_issue_dummy[-cgr$agents, ] <- 0
        true.matrix = matrix(true.values, nrow = length(agents), 
            ncol = simulation.control$n_issues, byrow = T)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        all.private.info = data.table(reshape2::melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim = data.table(pim)
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/{
            1/sqrt(incentive.matrix[, , t])
        })
        all.shared.info = data.table(reshape2::melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(shared.info.matrix[cgr$agents, 
            , t])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            do.call(rbind, replicate(length(agents), cgr.payout.estimates, 
                simplify = F)))/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses[, , t] = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses[, , t]
        expected.contrib.matrix[, , t] <- ctbs
        orig.contrib.matrix[, , t] <- as.matrix(ctbs * draws)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(incentive.matrix[cgr$agents, 
            cgr$issues, t])
        total.cgr.contrib.expectation = do.call(rbind, lapply(seq_along(agents), 
            function(x) colSums(issue.matrix[, , t] * agent_issue_dummy * 
                orig.contrib.matrix[, , t] * incentive.matrix[, 
                , t] * t(replicate(length(agents), agent.best.guesses[x, 
                , t])))))
        individual.cgr.payout.expectation = total.cgr.contrib.expectation/length(cgr$agents)
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses[, 
            , t] * incentive.matrix[, , t] < payout.matrix[, 
            , t]
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- individual.cgr.payout.expectation[cgr$agents, 
            cgr$issues] < payout.matrix[cgr$agents, cgr$issues, 
            t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues, t] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.error = sum((agent.best.guesses[cgr$agents, cgr$issues, 
            t] - true.matrix[cgr$agents, cgr$issues])^2)/length(true.matrix[cgr$agents, 
            cgr$issues])
        dynamics.tracker$principled.engagement[t] <- 1/cgr.error
        issue_cosim = cosine(t(issue.matrix[cgr$agents, cgr$issues, 
            t]))
        dynamics.tracker$issue.alignment[t] <- mean(issue_cosim[upper.tri(issue_cosim)])
        contrib.error = sum((contrib.matrix[cgr$agents, cgr$issues, 
            t] - expected.contrib.matrix[cgr$agents, cgr$issues, 
            t])^2)/length(contrib.matrix[cgr$agents, cgr$issues, 
            t])
        dynamics.tracker$shared.motivation[t] <- contrib.error
        dynamics.tracker$cgr.contributions[t] <- sum(contrib.matrix[cgr$agents, 
            cgr$issues, t])
        no.reallocate.payout = sum(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues, t] * incentive.matrix[cgr$agents, cgr$issues, 
            t])
        cgr.reallocate.payout = sum(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues, 
            t] * incentive.matrix[cgr$agents, cgr$issues, t])
        cgr.reallocate.payout - no.reallocate.payout
        dynamics.tracker$capacity.for.joint.action[t] <- sum((contrib.matrix[cgr$agents, 
            cgr$issues, t] - original.contrib)^2)/length(original.contrib)
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[78]]$visible
[1] FALSE


> 
> incentive.set = list(c(0.1,0.5),c(0.3,0.7),c(0.5,0.9),c(0.1,0.9))
> 
> res = foreach(i = 1:reps) %dopar% {
+   simulation.control = list(stakeholders = 50,regulators = 0, convenors = 0 ,
+                             incentive.range  = runif(n = 1,min = 0.1,max = 0.9),
+                            # skill.set = runif(1,0.1,0.9),
+                             #capacity.set = runif(1,0.1,0.9),
+                             uncertainty = runif(1,min = 0.25,2.25), 
+                             n_pieces = 1,
+                             min.payout = 0,
+                             max.payout = 10,n_issues = 100,
+                             number.of.issues.to.join = 2,
+                             beta = 0.8,alpha = 0.2, 
+                             t = 50,perturb.time = 15,perturb.type = NULL,
+                             CGselector='betweenness',behavior = "consistent")
+   tryCatch({EmersonScottModel(simulation.control = simulation.control,debug = F)},error = function(e) NULL)
+ }
Error in unserialize(socklist[[n]]) : error reading from connection
Calls: %dopar% ... recvOneData -> recvOneData.SOCKcluster -> unserialize
Execution halted
