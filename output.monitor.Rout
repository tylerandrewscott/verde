
R version 3.6.3 (2020-02-29) -- "Holding the Windsock"
Copyright (C) 2020 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> 
> require(pbapply)
Loading required package: pbapply
> source('code/model/emersonscott_model_3skill.R')
Loading required package: data.table
Loading required package: tidyverse
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──
✓ ggplot2 3.3.3     ✓ purrr   0.3.4
✓ tibble  3.1.2     ✓ dplyr   1.0.2
✓ tidyr   1.1.2     ✓ stringr 1.4.0
✓ readr   1.4.0     ✓ forcats 0.5.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
x dplyr::between()   masks data.table::between()
x dplyr::filter()    masks stats::filter()
x dplyr::first()     masks data.table::first()
x dplyr::lag()       masks stats::lag()
x dplyr::last()      masks data.table::last()
x purrr::transpose() masks data.table::transpose()
Loading required package: lsa
Loading required package: SnowballC
Loading required package: statnet
Loading required package: tergm
Loading required package: ergm
Loading required package: network
network: Classes for Relational Data
Version 1.16.1 created on 2020-10-06.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Martina Morris, University of Washington
                    Skye Bender-deMoll, University of Washington
 For citation information, type citation("network").
 Type help("network-package") to get started.


ergm: version 3.11.0, created on 2020-10-14
Copyright (c) 2020, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, UNSW Sydney
                    Martina Morris, University of Washington
                    with contributions from
                    Li Wang
                    Kirk Li, University of Washington
                    Skye Bender-deMoll, University of Washington
                    Chad Klumb
                    Michał Bojanowski, Kozminski University
                    Ben Bolker
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm").

NOTE: Versions before 3.6.1 had a bug in the implementation of the bd()
constraint which distorted the sampled distribution somewhat. In
addition, Sampson's Monks datasets had mislabeled vertices. See the
NEWS and the documentation for more details.

NOTE: Some common term arguments pertaining to vertex attribute and
level selection have changed in 3.10.0. See terms help for more
details. Use ‘options(ergm.term=list(version="3.9.4"))’ to use old
behavior.

Loading required package: networkDynamic

networkDynamic: version 0.10.1, created on 2020-01-16
Copyright (c) 2020, Carter T. Butts, University of California -- Irvine
                    Ayn Leslie-Cook, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Skye Bender-deMoll, University of Washington
                    with contributions from
                    Zack Almquist, University of California -- Irvine
                    David R. Hunter, Penn State University
                    Li Wang
                    Kirk Li, University of Washington
                    Steven M. Goodreau, University of Washington
                    Jeffrey Horner
                    Martina Morris, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("networkDynamic").


tergm: version 3.7.0, created on 2020-10-15
Copyright (c) 2020, Pavel N. Krivitsky, UNSW Sydney
                    Mark S. Handcock, University of California -- Los Angeles
                    with contributions from
                    David R. Hunter, Penn State University
                    Steven M. Goodreau, University of Washington
                    Martina Morris, University of Washington
                    Nicole Bohme Carnegie, New York University
                    Carter T. Butts, University of California -- Irvine
                    Ayn Leslie-Cook, University of Washington
                    Skye Bender-deMoll
                    Li Wang
                    Kirk Li, University of Washington
                    Chad Klumb
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("tergm").

Loading required package: ergm.count

ergm.count: version 3.4.0, created on 2019-05-15
Copyright (c) 2019, Pavel N. Krivitsky, University of Wollongong
                    with contributions from
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm.count").

NOTE: The form of the term ‘CMP’ has been changed in version 3.2 of
‘ergm.count’. See the news or help('CMP') for more information.

Loading required package: sna
Loading required package: statnet.common

Attaching package: ‘statnet.common’

The following object is masked from ‘package:base’:

    order

sna: Tools for Social Network Analysis
Version 2.6 created on 2020-10-5.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
 For citation information, type citation("sna").
 Type help(package="sna") to get started.

Loading required package: tsna

statnet: version 2019.6, created on 2019-06-13
Copyright (c) 2019, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Skye Bender-deMoll
                    Martina Morris, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("statnet").


There are updates for the following statnet packages on CRAN:
     Installed ReposVer Built  
tsna "0.3.1"   "0.3.3"  "3.6.3"
Restart R and use "statnet::update_statnet()" to get the updates.
Loading required package: igraph

Attaching package: ‘igraph’

The following objects are masked from ‘package:sna’:

    betweenness, bonpow, closeness, components, degree, dyad.census,
    evcent, hierarchy, is.connected, neighborhood, triad.census

The following objects are masked from ‘package:network’:

    %c%, %s%, add.edges, add.vertices, delete.edges, delete.vertices,
    get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite,
    is.directed, list.edge.attributes, list.vertex.attributes,
    set.edge.attribute, set.vertex.attribute

The following objects are masked from ‘package:dplyr’:

    as_data_frame, groups, union

The following objects are masked from ‘package:purrr’:

    compose, simplify

The following object is masked from ‘package:tidyr’:

    crossing

The following object is masked from ‘package:tibble’:

    as_data_frame

The following objects are masked from ‘package:stats’:

    decompose, spectrum

The following object is masked from ‘package:base’:

    union

Loading required package: multinets
multinets was developed by Neylson Crepalde as an extension to 'igraph'
Loading required package: tidygraph

Attaching package: ‘tidygraph’

The following object is masked from ‘package:igraph’:

    groups

The following object is masked from ‘package:stats’:

    filter

Loading required package: intergraph
> require(parallel)
Loading required package: parallel
> require(doParallel)
Loading required package: doParallel
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
> 
> cores = floor(detectCores() / 1.2)
> cl = makeCluster(cores)
> reps = 1000
> registerDoParallel(cl)
> 
> clusterEvalQ(cl,expr =  source('code/model/emersonscott_model.R'))
[[1]]
[[1]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[1]]$visible
[1] FALSE


[[2]]
[[2]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[2]]$visible
[1] FALSE


[[3]]
[[3]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[3]]$visible
[1] FALSE


[[4]]
[[4]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[4]]$visible
[1] FALSE


[[5]]
[[5]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[5]]$visible
[1] FALSE


[[6]]
[[6]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[6]]$visible
[1] FALSE


[[7]]
[[7]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[7]]$visible
[1] FALSE


[[8]]
[[8]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[8]]$visible
[1] FALSE


[[9]]
[[9]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[9]]$visible
[1] FALSE


[[10]]
[[10]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[10]]$visible
[1] FALSE


[[11]]
[[11]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[11]]$visible
[1] FALSE


[[12]]
[[12]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[12]]$visible
[1] FALSE


[[13]]
[[13]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[13]]$visible
[1] FALSE


[[14]]
[[14]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[14]]$visible
[1] FALSE


[[15]]
[[15]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[15]]$visible
[1] FALSE


[[16]]
[[16]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[16]]$visible
[1] FALSE


[[17]]
[[17]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[17]]$visible
[1] FALSE


[[18]]
[[18]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[18]]$visible
[1] FALSE


[[19]]
[[19]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[19]]$visible
[1] FALSE


[[20]]
[[20]]$value
function (simulation.control = NULL, debug = F) 
{
    agents = createAgents(simulation.control = simulation.control)
    true.values = createIssues(simulation.control = simulation.control)
    agent.motivation = createMotivation(simulation.control = simulation.control, 
        agents = agents)
    agent.issue.incidence.matrix = matrix(0 + (runif(simulation.control$n_issues * 
        length(agents)) > 0.75), ncol = simulation.control$n_issues)
    issue.homophily <- lsa::cosine(t(agent.issue.incidence.matrix))
    agent.homophily <- lsa::cosine(agent.issue.incidence.matrix)
    seed_network = network(rgraph(length(agents), tprob = 0.04), 
        directed = F)
    sqrt.agent.homophily = sqrt(agent.homophily)
    agent.network = (simulate(seed_network ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.agent.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    seed_network2 = network(rgraph(length(true.values), tprob = 0.02), 
        directed = F)
    sqrt.issue.homophily = sqrt(issue.homophily)
    issue.network = (simulate(seed_network2 ~ edges + isolates + 
        gwdegree(0.5, fixed = T) + gwesp(0.5, fixed = T) + edgecov(sqrt.issue.homophily), 
        coef = c(-2, -Inf, -0.75, 0.75, 1), constraints = ~edges))
    agent.edges = melt(as.sociomatrix(agent.network))
    issue.edges = melt(as.sociomatrix(issue.network))
    issue.edges$Var1 <- issue.edges$Var1 + length(agents)
    issue.edges$Var2 <- issue.edges$Var2 + length(agents)
    issue.agent.edges = data.table(melt(agent.issue.incidence.matrix))[value > 
        0, ]
    issue.agent.edges$Var2 <- issue.agent.edges$Var2 + length(agents)
    complete.edges = rbindlist(list(agent.edges, issue.edges, 
        issue.agent.edges))[value == 1 & Var1 != Var2, ]
    cgr = createCGR(select = simulation.control$CGselector, issue.net = issue.network, 
        number.to.join = simulation.control$number.of.issues.to.join, 
        agent.issue.graph = agent.issue.incidence.matrix)
    joint.action.matrix = principled.engagement.matrix = shared.info.matrix = private.info.matrix = issue.matrix = incentive.matrix = contrib.matrix = orig.contrib.matrix = payout.matrix = orig.payout.matrix = reciprocity.matrix = array(NA, 
        dim = list(length(agents), length(true.values), simulation.control$t))
    contributors.matrix = array(NA, dim = list(1, length(true.values), 
        simulation.control$t))
    incentive.matrix[, , 1] = joint.action.matrix[, , 1] = principled.engagement.matrix[, 
        , 1] = agent.motivation
    issue.matrix[, , 1] <- agent.issue.incidence.matrix
    dynamics.tracker = list(principled.engagement = NULL, capacity.for.joint.action = NULL, 
        shared.motivation = NULL)
    for (t in 1:simulation.control$t) {
        info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
            ag = agents, u = simulation.control$uncertainty, 
            n_pieces = simulation.control$n_pieces), simplify = F)
        private.info.matrix[, , t] <- do.call(rbind, info.pieces)
        if (t > 1) {
            private.info.matrix[, , t][orig.contrib.matrix[, 
                , t - 1] == 0] <- NA
        }
        all.private.info = data.table(melt(private.info.matrix))
        setnames(all.private.info, c("Var1", "Var2"), c("Agent", 
            "Issue"))
        all.private.info = all.private.info[!is.na(value), ]
        all.private.info[, `:=`(r, rank(-Var3)), by = .(Agent, 
            Issue)]
        all.private.info = all.private.info[r %in% 1:3, ]
        pim = dcast(all.private.info[, mean(value, na.rm = T), 
            by = .(Agent, Issue)], Agent ~ Issue, value.var = "V1")
        pim[, `:=`(Agent, NULL)]
        shared.info.matrix[, , t] = as.matrix(pim/sqrt(incentive.matrix[, 
            , t]))
        cgr.info = pim/sqrt(principled.engagement.matrix[, , 
            t])
        all.shared.info = data.table(melt(shared.info.matrix[, 
            , t]))
        setnames(all.shared.info, c("Var1", "Var2"), c("Sharing.Agent", 
            "Issue"))
        all.shared.info = all.shared.info[, mean(value, na.rm = T), 
            by = .(Sharing.Agent, Issue)]
        agent.edges = data.table(agent.edges)
        agent.alters = lapply(seq_along(agents), function(a) agent.edges[value == 
            1 & Var1 == a, ]$Var2)
        personal.network.info.t = do.call(rbind, lapply(seq(length(agents)), 
            function(x) all.shared.info[Sharing.Agent %in% agent.alters[[x]], 
                mean(V1, na.rm = T), by = .(Issue)]$V1))
        cgr.payout.estimates = colMeans(cgr.info[cgr$agents, 
            ])
        info.in.cgr = as.matrix((pim + personal.network.info.t + 
            cgr.payout.estimates)/3)
        info.out.cgr = as.matrix((pim + personal.network.info.t)/2)
        operating.info = do.call(rbind, lapply(seq(length(agents)), 
            function(a) {
                if (a %in% cgr$agents) {
                  info.in.cgr[a, ]
                }
                else {
                  info.out.cgr[a, ]
                }
            }))
        agent.best.guesses = operating.info
        draws = do.call(rbind, lapply(seq_along(agents), function(a) rbinom(n = simulation.control$n_issues, 
            size = 1, prob = incentive.matrix[a, , t])))
        ctbs = incentive.matrix[, , t] * issue.matrix[, , t] * 
            agent.best.guesses * draws
        orig.contrib.matrix[, , t] <- as.matrix(ctbs)
        orig.contrib.matrix[, , t][is.na(orig.contrib.matrix[, 
            , t])] <- 0
        cgr.reallocate.contrib = t(sapply(cgr$agents, function(a) sum(orig.contrib.matrix[a, 
            cgr$issues, t]) * {
            cgr.payout.estimates[cgr$issues]/sum(cgr.payout.estimates[cgr$issues])
        }))
        original.contrib = orig.contrib.matrix[cgr$agents, cgr$issues, 
            t]
        cgr.contribs = (original.contrib * (1 - colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t]))) + cgr.reallocate.contrib * colMeans(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        contrib.matrix[, , t] <- orig.contrib.matrix[, , t]
        contrib.matrix[cgr$agents, cgr$issues, t] <- cgr.contribs
        payoff.by.issue.t = colSums(contrib.matrix[, , t] * true.values)
        split.by.issue.t = payoff.by.issue.t/colSums(issue.matrix[, 
            , t])
        payout.matrix[, , t] = matrix(rep(split.by.issue.t, each = length(agents)), 
            byrow = F, nrow = length(agents)) * issue.matrix[, 
            , t]
        reciprocity.matrix[, , t] = contrib.matrix[, , t] * agent.best.guesses * 
            incentive.matrix[, , t] < payout.matrix[, , t]
        contributors.matrix[, , t] <- colSums(orig.contrib.matrix[, 
            , t] > 0, na.rm = T)
        cgr.reciprocity = rowSums(contrib.matrix[cgr$agents, 
            cgr$issues, t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t]) < rowSums(payout.matrix[cgr$agents, 
            cgr$issues, t])
        reciprocity.matrix[cgr$agents, cgr$issues, t] <- cgr.reciprocity
        cgr.better = rowSums(original.contrib * agent.best.guesses[cgr$agents, 
            cgr$issues] * incentive.matrix[cgr$agents, cgr$issues, 
            t]) < rowSums(contrib.matrix[cgr$agents, cgr$issues, 
            t] * agent.best.guesses[cgr$agents, cgr$issues] * 
            incentive.matrix[cgr$agents, cgr$issues, t])
        dynamics.tracker$shared.motivation[t] <- mean(reciprocity.matrix[cgr$agents, 
            cgr$issues, t])
        dynamics.tracker$capacity.for.joint.action[t] <- mean(joint.action.matrix[cgr$agents, 
            cgr$issues, t])
        pe <- min(sum(incentive.matrix[cgr$agents, cgr$issues, 
            t] * issue.matrix[cgr$agents, cgr$issues, t])/sum(orig.contrib.matrix[cgr$agents, 
            cgr$issues, t] > 0), 1)
        dynamics.tracker$principled.engagement[t] <- mean(principled.engagement.matrix[, 
            , t])
        recip = Reduce("+", lapply(max(1, t - 3):max(1, t), function(x) (reciprocity.matrix[, 
            , x] + 0)))/t
        if (t < simulation.control$t) {
            issue.matrix[, , t + 1] <- issue.matrix[, , t]
            incentive.matrix[, , t + 1] <- incentive.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                recip
            principled.engagement.matrix[, , t + 1] <- principled.engagement.matrix[, 
                , t] * simulation.control$beta + simulation.control$alpha * 
                pe
            joint.action.matrix[cgr$agents, cgr$issues, t + 1] <- joint.action.matrix[cgr$agents, 
                cgr$issues, t] * simulation.control$beta + simulation.control$alpha * 
                mean(cgr.better)
        }
        if (t == simulation.control$perturb.time & !is.null(simulation.control$perturb.type)) {
            if (simulation.control$perturb.type == "payoff.change") {
                new.true.values = createIssues(simulation.control = simulation.control)
                new.info.pieces = replicate(length(agents), createAgentInformationMatrix(payoffs = true.values, 
                  ag = agents, u = simulation.control$uncertainty, 
                  n_pieces = simulation.control$n_pieces), simplify = F)
                true.values[cgr$issues] <- new.true.values[cgr$issues]
            }
            if (simulation.control$perturb.type == "lose.contributor") {
                cgr$dropped.agent = sample(x = cgr$agents, size = 1, 
                  prob = rowSums(contrib.matrix[cgr$agents, , 
                    t]))
                cgr$agents = cgr$agents[cgr$agents != cgr$dropped.agent]
            }
            if (simulation.control$perturb.type == "add.agents") {
                cgr$new_members = which(!seq(length(agents)) %in% 
                  cgr$agents)[rbinom(size = 1, n = length(which(!seq(length(agents)) %in% 
                  cgr$agents)), prob = 0.25) == 1]
                cgr$agents = union(cgr$agents, cgr$new_members)
            }
        }
    }
    if (!debug) {
        return(list(payoffs = true.values, cgr = cgr, dynamics = dynamics.tracker, 
            issue_sd = summary(apply(private.info.matrix[, , 
                1], 2, sd)[cgr$issues]), sim = simulation.control, 
            cgr.payout.t = apply(payout.matrix[, cgr$issues, 
                ], 3, sum), starting.incentive = summary(incentive.matrix[, 
                1, 1])))
    }
    if (debug) {
        return(list(incentive.matrix = incentive.matrix, cgr = cgr, 
            mults = true.values, joint.action = joint.action.matrix, 
            principled.engagement = principled.engagement.matrix, 
            contribs = contrib.matrix, payouts = payout.matrix, 
            sim = simulation.control))
    }
}

[[20]]$visible
[1] FALSE


> 
> incentive.set = list(c(0.1,0.5),c(0.3,0.7),c(0.5,0.9),c(0.1,0.9))
> 
> res = foreach(i = 1:reps) %dopar% {
+   simulation.control = list(stakeholders = 50,regulators = 0, convenors = 0 ,
+                             incentive.set = sample(incentive.set,1)[[4]],
+                             uncertainty = runif(1,min = 0.25,2.25), 
+                             n_pieces = 1,
+                             min.payout = 0,
+                             max.payout = 10,n_issues = 100,
+                             number.of.issues.to.join = 2,
+                             beta = 0.9,alpha = 0.1, 
+                             t = 50,perturb.time = 15,perturb.type = NULL,
+                             CGselector='betweenness',behavior = "consistent")
+   tryCatch({EmersonScottModel(simulation.control = simulation.control)},error = function(e) NULL)
+ }
Error in { : task 1 failed - "subscript out of bounds"
Calls: %dopar% -> <Anonymous>
Execution halted
